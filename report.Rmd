---
title: "Midterm project, group 6"
author: "Ruoyuan Qian"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(ggplot2)
library(Rmisc)
library(corrplot)
library(FNN)
library(pdp)
library(earth)
library(sandwich)
library(stargazer)
load("report.RData")
```

# 1	Introduction

## 1.1	Background

In the dataset, the response variable is the SalePrice ($) of residential homes in Ames, Iowa. And there are 1460 observations with 79 explanatory variables describing a variety of aspects of these housings. Among explanatory variables, there are 37 numeric variables and 43 categorical variables. 

## 1.2 Objective

In this report, the focus is among eight methods (multiple linear regression, ridge regression, lasso regression, elastic regression, principal component regression (PCR), k-nearest neighbors algorithm (k-NN), generalized additive model (GAM), multivariate adaptive regression spline (MARS)), which one is the best to predict sale price of house for the particular dataset and which predictors are most influential for the response SalePrice.

## 1.3	Exploratory data analysis (EDA)

### 1.3.1	Data cleaning

Missing value is checked for each predictor and predictors with the number of missing value greater than 500 are excluded from the dataset. At the meantime, predictors with many zeros or near-zero observations are removed as well.  Finally, NA’s are dropped from the remaining data. When all works are done, there are 55 predictors in total including 28 numeric predictors and 27 categorical predictors. 

### 1.3.2	Visualization

The distribution of response SalePrice ($) is checked (Fig. 1), as we can see, it is continuous variable with a right skewed shape. Since all methods in report do not need normal distribution assumption, so the original value of response can be used in model fitting. 

Scatter plots are checked for numeric variables (Fig. 2), bar plots are shown for categorical variables (Fig. 3). Correlations between predictors are visualized by heat plot (Fig. 4). 

# 2	Methods

The data for all methods is the same to keep the ability of comparison. Data is scaled and standardized in model fitting. All categorical variables are transformed to factor variavles. 

## 2.1	Linear Methods

### 2.1.1	Multiple linear regression (MLR)

$$Y = \beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p+\varepsilon$$
$\beta’s$ are estimated by least squared estimation:
$$RSS=\textstyle\sum_{i=1}^{n}(y_i-\widehat{y_i})$$,
where $\widehat{Y} = \widehat{\beta_0}+\widehat{\beta_1}\widehat{X_1}+…+\widehat{\beta_p}\widehat{X_p}$. 
Although MLR requires a Gaussian error for any inference, the report is focus on the ability of prediction, so we don’t have to do transformation for response variable.

### 2.1.2	Ridge regression

The ridge coefficient estimation is the minimium of the loss function:
$$min(RSS+\lambda\textstyle\sum_{j=1}^{p}\beta^2_j) $$
All coefficients will shrink when $\lambda$ increases, but none of them will shrink to zero. So ridge regression will remain all predictors in the final model.

### 2.1.3	LASSO regression

The LASSO coefficient estimation is the minimum of the loss function:
$$min(RSS+\lambda\textstyle\sum_{j=1}^{p}|\beta_j|) $$
All coefficients will shrink to zero when $\lambda$ is large enough. LASSO regression will remain a subset of predictors in the final model.

### 2.1.4	Elastic regression

The elastic coefficient estimation is the minimum of the loss function:
$$min(RSS+\lambda_1\textstyle\sum_{i=1}^{n}\beta^2_j+\lambda_2\textstyle\sum_{i=1}^{n}|\beta_j|) $$
All coefficients will not shrink to exact zero when $\lambda$ increases. In R, $\alpha$ can be changed in [0,1], so combinations of a range of $\lambda$ and $\alpha$ is implemented to find the best combination of tuning parameters with the criteria of smallest MSE through cross validation (CV).

### 2.1.5	Principal component regression (PCR)

It includes two steps, dimension reduction and regression. 
$$Z_m=\textstyle\sum_{j=1}^{p}\phi_{mj}X_{j}$$
$$y_i=\theta_0+\textstyle\sum_{m=1}^{M}\theta_mz_{im}$$

## 2.2	Non-linear methods

### 2.2.1	K-nearest neighbors algorithm (k-NN)

The k nearest points are used to fit the line. 
$$\hat{f}(x_0)=Ave(y|x\in N(x_0))=\textstyle\sum_{i=1}^{n}w(x_0,x_i)y_i$$
Where $w(x,x_i)=I(x_i\in N_k(x))/K$

### 2.2.2	Generalized additive model (GAM)

It allows flexible non-linearities in several variables based on their own scatter plot or degree of freedom (DF), if the points are not linear shaped or the DF is greater than 1, then a non-linear term should be considered.
$$g[E(y|X)]=\beta_0+f_1(X_1)+...+f_p(X_p)$$

### 2.2.3	Multivariate Adaptive Regression Spline (MARS)

It is a piecewise linear model while the cut points are selected by algorithm, and then the hinge functions can be written as $(h(x-c),h(c-x))$.

# 3	Results

## 3.1	Model comparison

For LASSO, plot of MSE across a sequence of $\lambda$ is made (Fig. 5), and the best $\lambda$ is $e^{7.035}$. For ridge model, plot of MSE across a sequence of $\lambda$ (Fig. 6) shows that the best tuning parameter is $\lambda = e^{10.10}$. As for elastic model, 750 combinations of $\alpha$ and $\lambda$ are checked (Fig. 7), the best pair is $\alpha = 0, \lambda = e^{10.10}$, which is the same as ridge, so in the model comparison, only ridge regression will be presented. For PCR, 54 principle components (PC) are tested, and the best number of principle component (PC) is 26 through MSE (Fig. 8). 

For k-NN model, after testing a sequence of k from 5 to 43, the best tuning parameter is equal to 11. For GAM model, “train” function is implemented and 20 out of 54 predictors are tested for non-linear relation to response (Fig. 9). For MARS model, 10 cut points are used to fit the model (Fig. 10).

All models are compared through MSE (Tab. 1, Fig 11, Fig. 12). The best model is the one with the smallest MSE, which is MARS. K-nn method obtain the largest MSE among the all.


## 3.2	Model interpretation

There are 10 cut points in MARS model, they are in "OverallQual", "GrLivArea", "X2ndFlrSF", "YearBuilt", "BsmtFinSF1", "LotArea", "OverallCond", "X1stFlrSF", "TotalBsmtSF", "SaleCondition". The coefficients of hinge functions are shown in Tab. 2. According to Fig. 10, except “X1stFlrSF” and “SaleCondition”, all predictors have increasing trends when response rises. “X1stFlrSF” has a dereaseing trend all the time when the sale price increases. 

Moreover, the top 10 most important variables for the MARS model are checked in Tab.3, for a decreasing order of contribution, they are: “OverallQual”, “GrLivArea”, “YearBuilt”, “BsmtFinSF1”, “X1stFlrSF”, “X2ndFlrSF”, “OverallCond”, “LotArea”, “TotalBsmtSF”, “SaleCondition”.

Heat plot for the top 10 the most important variables is made (Fig. 13), “X2ndFlrSF” and “GrLivArea”, “X1stFlrSF” and “TotalBsmtSF”, “YearBuilt” and “OverallQual” are highly correlated, respectively. 

# 4	Discussion

As for data interpretation, since the data from the Kaggle does not include the specific meaning for each variable, so I cannot interpret the model in a more detailed way. Besides, the data contains a lot of integer variables, I treated them as continuous variables, so gaps are introduced in some of the scatter plots. 

\pagebreak
# Figures and Tables

## Table 1 MSE of all methods through cross validation
```{r echo=FALSE, warning=FALSE, message=FALSE}
resamp$values %>% 
  as.data.frame() %>% 
  janitor::clean_names() %>% 
  select(Lm = lm_rmse, LASSO = lasso_rmse, Ridge = ridge_rmse, 
         PCR = pcr_rmse, Knn = knn_rmse, GAM = gam_rmse, MARS = mars_rmse
         ) %>% broom::tidy() %>% 
  select(-n,-trimmed,-mad,-skew,-kurtosis,-se)%>% 
  knitr::kable() 

```

## Table 2 Hinge functions and their coefficients in MARS model

```{r echo=FALSE, warning=FALSE, message=FALSE}
mars.fit$finalModel$coefficients %>% 
  as.data.frame()%>%
  select("Coefficient" = y) %>% 
  knitr::kable() 
```

## Table 3 Top 10 most important variables in MARS model

```{r echo=FALSE, warning=FALSE, message=FALSE}
im_var = varImp(mars.fit) 
im_var$importance %>% 
  head(10) %>% 
  knitr::kable() 
```

## Figure 1 Distridution of response (SalePrice)
```{r echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
density_sale
```

## Figure 2 Scatter plots of continuous predictors against SalePrice
```{r  echo=FALSE, warning=FALSE, message=FALSE,fig.height=15,fig.width=15}

numeric_var_index = 
 final_house%>% 
  map(.,is.numeric) %>% 
  unlist() %>% 
  as.vector()

x <- model.matrix(SalePrice~.,
                  final_house[,which(numeric_var_index == TRUE)])[,-1]
y <- final_house$SalePrice


theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
scatter_plot = 
  featurePlot(x, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(7, 4))

scatter_plot
```

## Figure 3 Bar plots of categorical predictors
```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=50,fig.width=40}
multiplot( 
 plots(dataframe %>% select( 1 )) , 
plots(dataframe %>% select( 2 )) , 
plots(dataframe %>% select( 3 )) , 
plots(dataframe %>% select( 4 )) , 
plots(dataframe %>% select( 5 )) , 
plots(dataframe %>% select( 6 )) , 
plots(dataframe %>% select( 7 )) , 
plots(dataframe %>% select( 8 )) , 
plots(dataframe %>% select( 9 )) , 
plots(dataframe %>% select( 10 )) ,
plots(dataframe %>% select( 11 )) ,
plots(dataframe %>% select( 12 )) ,
plots(dataframe %>% select( 13 )) ,
plots(dataframe %>% select( 14 )) ,
plots(dataframe %>% select( 15 )) ,
plots(dataframe %>% select( 16 )) ,
plots(dataframe %>% select( 17 )) ,
plots(dataframe %>% select( 18 )) ,
plots(dataframe %>% select( 19 )) ,
plots(dataframe %>% select( 20 )) ,
plots(dataframe %>% select( 21 )) ,
plots(dataframe %>% select( 22 )) ,
plots(dataframe %>% select( 23 )) ,
plots(dataframe %>% select( 24 )) ,
plots(dataframe %>% select( 25 )) ,
plots(dataframe %>% select( 26 )) ,
          cols=4) 
```

## Figure 4 Heat map for all predictors

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=20,fig.width=20}
reg_data = as.data.frame(map(final_house,as.numeric))

corrplot(cor(reg_data %>% select(-SalePrice)),title = "Correlation Plot", method = "square", addgrid.col = "darkgray", order="hclust", mar = c(4,0,4,0), addrect = 6, rect.col = "black", rect.lwd = 5,cl.pos = "b", tl.col = "indianred4", tl.cex = 1.5, cl.cex = 1.5)
```

## Figure 5 LASSO

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
LASSO_plot
```


## Figure 6 Ridge

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
ridge_plot
```


## Figure 7 Elastic

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
elastic_plot
```


## Figure 8 PCR

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
pcr_plot
```

## Figure 9 GAM

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=15,fig.width=15}
par(mfcol = c(5,4))
plot(gam.fit$finalModel)
```

## Figure 10 MARS

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=20,fig.width=15}
plot_mars = function(name){
  p1 <- partial(mars.fit, pred.var = c(name), grid.resolution = 10) %>% autoplot()
  p1
}

cut_name = c("OverallQual","GrLivArea","X2ndFlrSF","YearBuilt","BsmtFinSF1",
             "LotArea","OverallCond","X1stFlrSF","TotalBsmtSF","SaleCondition")

plot_list = map(cut_name,plot_mars)

multiplot(plotlist = plot_list, cols=2)

```

## Figure 11 Box plot of all methods through CV

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
 bwplot(resamp, metric = "RMSE")

```

## Figure 12 Box plot of all methods through CV

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
 parallelplot(resamp, metric = "RMSE")

```


## Figure 13 Heat map for top 10 most important variables

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=15,fig.width=15}

corrplot(cor(reg_data %>% select(-SalePrice) %>% select(cut_name)),title = "Correlation Plot", method = "square", addgrid.col = "darkgray", order="hclust", mar = c(4,0,4,0), addrect = 5, rect.col = "black", rect.lwd = 5,cl.pos = "b", tl.col = "indianred4", tl.cex = 1.5, cl.cex = 1.5)
```