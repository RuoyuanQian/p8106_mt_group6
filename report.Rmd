---
title: "Prediction of sale price for housing"
author: "Ruoyuan Qian"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(ggplot2)
library(Rmisc)
library(corrplot)
library(FNN)
library(pdp)
library(earth)
library(sandwich)
library(stargazer)
load("report_0402.RData")
```

# 1	Introduction

## 1.1 Objective

In this report, the focus is among eight methods (multiple linear regression, ridge regression, lasso regression, elastic regression, principal component regression (PCR), k-nearest neighbors algorithm (k-NN), generalized additive model (GAM), multivariate adaptive regression spline (MARS)), which one is the best to predict sale price of house for the particular dataset and which predictors are most influential for the response SalePrice.

## 1.2	Data cleaning

Missing value is checked for each predictor and predictors with the number of missing value greater than 500 are excluded from the dataset. At the meantime, predictors with many zeros or near-zero observations are removed as well. Then NA’s are dropped from the remaining data. 

# 2	Exploratory data analysis (EDA)

The distribution of response SalePrice ($) is checked (Fig. 1), as we can see, it is continuous variable with a right skewed shape. Since all methods in report do not need normal distribution assumption, so the original value of response can be used in model fitting. 

Scatter plots are checked for numeric variables (Fig. 2), since I treated integers as continuous variables, so gaps are introduced in some of the scatter plots, such as "GarageCars", "MoSold", "YrSold", "BsmtFullBath". There is a non-linear trend in "GarageYrBlt", "BsmtUnfSF", "YearBuilt".

Bar plots are shown for categorical variables (Fig. 3). Some categories are not equally distributed within each predictors, like " LotShape", "RoofStyle", "LotConfig".

Correlations between numeric predictors are visualized by heat plot (Fig. 4). 

# 3	Models

After data cleaning, there are 44 predictors in total including 28 numeric predictors and 16 categorical predictors. After that, data is transformed to model matrix and categorical predictors are transformed as dummy variables. Finally, zero and near zero variables are checked again and excluded in the model matrix. Therefore, the final data includes 52 predictors with 28 numeric predictors and 24 dummy variables. The final data is scaled and standardized in model fitting. 

Repeated cross validation (CV) is implemented in the report.

## 3.1	Linear Methods

### 3.1.1	Multiple linear regression (MLR)

$$Y = \beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p+\varepsilon$$
$\beta’s$ are estimated by least squared estimation:
$$RSS=\textstyle\sum_{i=1}^{n}(y_i-\widehat{y_i})$$
where $\widehat{Y} = \widehat{\beta_0}+\widehat{\beta_1}\widehat{X_1}+…+\widehat{\beta_p}\widehat{X_p}$. 
Although MLR requires a Gaussian error for any inference, the report is focus on the ability of prediction, so we don’t have to do transformation for response variable.

### 3.1.2	Ridge regression

The ridge coefficient estimation is the minimium of the loss function:
$$min(RSS+\lambda\textstyle\sum_{j=1}^{p}\beta^2_j) $$
All coefficients will shrink when $\lambda$ increases, but none of them will shrink to zero. So ridge regression will remain all predictors in the final model. In R, $\alpha=0$ is fixed, and a range of $\lambda$ is implemented to find the best tuning parameter with the criteria of smallest MSE through CV.

### 3.1.3	LASSO regression

The LASSO coefficient estimation is the minimum of the loss function:
$$min(RSS+\lambda\textstyle\sum_{j=1}^{p}|\beta_j|) $$
All coefficients will shrink to zero when $\lambda$ is large enough. LASSO regression will remain a subset of predictors in the final model. In R, $\alpha=1$ is fixed, selection of best tuning parameter is similar to ridge.

### 3.1.4	Elastic regression

The elastic coefficient estimation is the minimum of the loss function:
$$min(RSS+\lambda_1\textstyle\sum_{i=1}^{n}\beta^2_j+\lambda_2\textstyle\sum_{i=1}^{n}|\beta_j|) $$
All coefficients will not shrink to exact zero when $\lambda$ increases. In R, $\alpha$ can be changed in [0,1], so combinations of a range of $\lambda$ and $\alpha$ is implemented to find the best combination of tuning parameters with the criteria of smallest MSE through CV.

### 3.1.5	Principal component regression (PCR)

It includes two steps, dimension reduction and regression. 

$$Z_m=\textstyle\sum_{j=1}^{p}\phi_{mj}X_{j},y_i=\theta_0+\textstyle\sum_{m=1}^{M}\theta_mz_{im}$$

The number of principal components (CP) is chosen by CV with smallest MSE.

## 3.2	Non-linear methods

### 3.2.1	K-nearest neighbors algorithm (k-NN)

The k nearest points are used to fit the line. 
$$\hat{f}(x_0)=Ave(y|x\in N(x_0))=\textstyle\sum_{i=1}^{n}w(x_0,x_i)y_i$$
where $w(x,x_i)=I(x_i\in N_k(x))/K$. Tuning parameter, the number of nearset points k is chosen through CV.

### 3.2.2	Generalized additive model (GAM)

It allows flexible non-linearities in several variables based on their own scatter plot or degree of freedom (DF), if the points are not linear shaped or the DF is greater than 1, then a non-linear term should be considered.
$$g[E(y|X)]=\beta_0+f_1(X_1)+...+f_p(X_p)$$

### 3.2.3	Multivariate Adaptive Regression Spline (MARS)

It is a piecewise linear model while the cut points are selected by algorithm, and then the hinge functions can be written as $(h(x-c),h(c-x))$.

## 3.3	Results

For LASSO, plot of MSE across a sequence of $\lambda$ is made (Fig. 5), and the best $\lambda$ is $733.6191$. For ridge model, plot of MSE across a sequence of $\lambda$ (Fig. 6) shows that the best tuning parameter is $\lambda = 10575.88$. As for elastic model, 750 combinations of $\alpha$ and $\lambda$ are checked (Fig. 7), the best pair is $\alpha = 0, \lambda = 10673.4$, since $\alpha = 0$, the result is similar as ridge. For PCR, 52 principle components (PC) are tested, and the best number of principle component (PC) is 45 through smallest MSE (Fig. 8). 

For k-NN model, after testing a sequence of k from 5 to 43, the best tuning parameter is equal to 11. For GAM model, “train” function is implemented and 17 out of 52 variables are tested having a non-linear relationships with response (Fig. 9). For MARS model, 10 cut points are used to fit the model (Fig. 10).

# 4 Conclusion

All models are compared through MSE (Tab. 1, Fig 11, Fig. 12). With differnt metrics, the best model is different. GAM model obtained the smallest median MSE while MARS model obtained the smallest average MSE. Here we use the average MSE as the final criteria. The best model is MARS. 

There are 10 cut points in MARS model, they are in "OverallQual", "GrLivArea", "YearBuilt", "X1stFlrSF", "BsmtFinSF1", "X2ndFlrSF", "OverallCond", "LotArea", "TotalBsmtSF", "BedroomAbvGr". The coefficients of hinge functions are shown in Tab. 2. According to Fig. 10, except “X1stFlrSF”, “GrLivArea” and "BedroomAbvGr". All predictors have increasing trends when response rises. “X1stFlrSF” and "BedroomAbvGr" has a dereaseing trend all the time when the sale price increases. 

Moreover, the top 10 most important variables for the MARS model are checked in Tab.3, for a decreasing order of contribution, they are: "OverallQual", "GrLivArea", "YearBuilt", "X1stFlrSF", "BsmtFinSF1", "X2ndFlrSF", "OverallCond", "LotArea", "TotalBsmtSF", "BedroomAbvGr".

Heat plot for the top 10 the most important variables is made (Fig. 13), “X2ndFlrSF” and “GrLivArea”, “X1stFlrSF” and “TotalBsmtSF”, “YearBuilt” and “OverallQual” are highly correlated, respectively. 


\pagebreak
# Appendix - Figures and Tables

## Table 1 MSE of all methods through cross validation
```{r echo=FALSE, warning=FALSE, message=FALSE}
resamp$values %>% 
  as.data.frame() %>% 
  janitor::clean_names() %>% 
  select(Lm = lm_rmse, LASSO = lasso_rmse, Ridge = ridge_rmse, Elastic = elastic_rmse, PCR = pcr_rmse, Knn = knn_rmse, GAM = gam_rmse, MARS = mars_rmse
         ) %>% broom::tidy() %>% 
  select(-n,-trimmed,-mad,-skew,-kurtosis,-se)%>% 
  knitr::kable() 

```

## Table 2 Hinge functions and their coefficients in MARS model

```{r echo=FALSE, warning=FALSE, message=FALSE}
mars.fit$finalModel$coefficients %>% 
  as.data.frame()%>%
  select("Coefficient" = y) %>% 
  knitr::kable() 
```

## Table 3 Top 10 most important variables in MARS model

```{r echo=FALSE, warning=FALSE, message=FALSE}
im_var = varImp(mars.fit) 
im_var$importance %>% 
  head(10) %>% 
  knitr::kable() 
```

## Figure 1 Distridution of response (SalePrice)
```{r echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
density_sale
```

## Figure 2 Scatter plots of continuous predictors against SalePrice
```{r  echo=FALSE, warning=FALSE, message=FALSE,fig.height=15,fig.width=15}

numeric_var_index = 
 final_house%>% 
  map(.,is.numeric) %>% 
  unlist() %>% 
  as.vector()

x <- model.matrix(SalePrice~.,
                  final_house[,which(numeric_var_index == TRUE)])[,-1]
y <- final_house$SalePrice


theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
scatter_plot = 
  featurePlot(x, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(7, 4))

scatter_plot
```

## Figure 3 Bar plots of categorical predictors
```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=50,fig.width=40}
multiplot( 
 plots(dataframe %>% select( 1 )) , 
plots(dataframe %>% select( 2 )) , 
plots(dataframe %>% select( 3 )) , 
plots(dataframe %>% select( 4 )) , 
plots(dataframe %>% select( 5 )) , 
plots(dataframe %>% select( 6 )) , 
plots(dataframe %>% select( 7 )) , 
plots(dataframe %>% select( 8 )) , 
plots(dataframe %>% select( 9 )) , 
plots(dataframe %>% select( 10 )) ,
plots(dataframe %>% select( 11 )) ,
plots(dataframe %>% select( 12 )) ,
plots(dataframe %>% select( 13 )) ,
plots(dataframe %>% select( 14 )) ,
plots(dataframe %>% select( 15 )) ,
plots(dataframe %>% select( 16 )) ,
          cols=4) 
```

## Figure 4 Heat map for all predictors

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=20,fig.width=20}

corrplot(cor(final_house[,which(numeric_var_index == TRUE)] %>% select(-SalePrice)),title = "Correlation Plot", method = "square", addgrid.col = "darkgray", order="hclust", mar = c(4,0,4,0), addrect = 6, rect.col = "black", rect.lwd = 5,cl.pos = "b", tl.col = "indianred4", tl.cex = 1.5, cl.cex = 1.5)
```

## Figure 5 LASSO

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
LASSO_plot
```


## Figure 6 Ridge

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
ridge_plot
```


## Figure 7 Elastic

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
elastic_plot
```


## Figure 8 PCR

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
pcr_plot
```

## Figure 9 GAM

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=15,fig.width=15}
par(mfcol = c(5,4))
plot(gam.fit$finalModel)
```

## Figure 10 MARS

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=20,fig.width=15}
plot_mars = function(name){
  p1 <- partial(mars.fit, pred.var = c(name), grid.resolution = 10) %>% autoplot()
  p1
}

cut_name = c("OverallQual", "GrLivArea", "YearBuilt", "X1stFlrSF", "BsmtFinSF1", "X2ndFlrSF", "OverallCond", "LotArea", "TotalBsmtSF", "BedroomAbvGr")

plot_list = map(cut_name,plot_mars)

multiplot(plotlist = plot_list, cols=2)

```

## Figure 11 Box plot of all methods through CV

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
 bwplot(resamp, metric = "RMSE")

```

## Figure 12 Box plot of all methods through CV

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=4}
 parallelplot(resamp, metric = "RMSE")

```


## Figure 13 Heat map for top 10 most important variables

```{r,echo=FALSE, warning=FALSE, message=FALSE,fig.height=15,fig.width=15}

corrplot(cor(final_house %>% select(-SalePrice) %>% select(cut_name)),title = "Correlation Plot", method = "square", addgrid.col = "darkgray", order="hclust", mar = c(4,0,4,0), addrect = 5, rect.col = "black", rect.lwd = 5,cl.pos = "b", tl.col = "indianred4", tl.cex = 1.5, cl.cex = 1.5)
```




